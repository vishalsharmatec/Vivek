{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e7b3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI , GoogleGenerativeAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b900da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23b1e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load document\n",
    "loader = TextLoader(\"faq/account.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "665a25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f922a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q-Can I rejoin after canceling my membership?\\nAns-Yes, you can rejoin our panel even if you have canceled your membership in the past. That said, all memberships are governed by our community policies; please read the ‘Terms and Conditions’ of our community membership for details or email us at support@opinion-edge.com.\\n\\nQ-I canceled my membership after requesting reward redemption. Will I receive the rewards?\\nAns-Yes, if you cancel your membership after requesting reward redemption, your redemption request will still be processed. If you face any difficulty in processing, please email us at support@opinion-edge.com.\\n\\nQ-How can I upgrade to the next level?\\nAns-There are multiple criteria for level upgrading, encompassing factors such as user survey participation, the quality of responses, client rejection history, and no rejected redemptions. Once a user meets these criteria, they are automatically elevated to the next level.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[4].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968def2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'Document'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Embeddings (models/embedding-001)\u001b[39;00m\n\u001b[32m      2\u001b[39m embeddings = GoogleGenerativeAIEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mmodels/embedding-001\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m vector = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sharm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:209\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    207\u001b[39m embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    208\u001b[39m batch_start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_prepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[32m    211\u001b[39m         titles_batch = titles[\n\u001b[32m    212\u001b[39m             batch_start_index : batch_start_index + \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    213\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sharm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:133\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._prepare_batches\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m    126\u001b[39m current_text = texts[text_index]\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Number of tokens per a text is conservatively estimated\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# as 2 times number of words, punctuation and whitespace characters.\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Using `count_tokens` API will make batching too expensive.\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Utilizing a tokenizer, would add a dependency that would not\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# necessarily be reused by the application using this class.\u001b[39;00m\n\u001b[32m    132\u001b[39m current_text_token_cnt = (\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_split_by_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    134\u001b[39m     * \u001b[32m2\u001b[39m\n\u001b[32m    135\u001b[39m )\n\u001b[32m    136\u001b[39m end_of_batch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_text_token_cnt > _MAX_TOKENS_PER_BATCH:\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# Current text is too big even for a single batch.\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# Such request will fail, but we still make a batch\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# so that the app can get the error from the API.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sharm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:111\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._split_by_punctuation\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    109\u001b[39m pattern = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Using re.split to split the text based on the pattern\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [segment \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m segment]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sharm\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:267\u001b[39m, in \u001b[36msplit\u001b[39m\u001b[34m(pattern, string, maxsplit, flags, *args)\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    262\u001b[39m     warnings.warn(\n\u001b[32m    263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmaxsplit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is passed as positional argument\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'Document'"
     ]
    }
   ],
   "source": [
    "\n",
    "#Embeddings (models/embedding-001)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",)\n",
    "vector = embeddings.embed_documents(split_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b4852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
